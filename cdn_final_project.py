# -*- coding: utf-8 -*-
"""CDN-Final-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_WIrsP6bKXhDTmDqrxBrc69zygcwKOxX
"""

import networkx as nx
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random

from sklearn.metrics import classification_report
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn import svm
from numpy.core.fromnumeric import mean
from sklearn.neighbors import KNeighborsClassifier
from networkx.algorithms.community.label_propagation import label_propagation_communities

import networkx.algorithms.community as nx_comm
from networkx.algorithms.community import modularity
from networkx.algorithms.community.modularity_max import greedy_modularity_communities

from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.naive_bayes import GaussianNB

!pip install EoN
import EoN

"""# Step 1

### Reading the files and extracting the data
"""

# nodes file
nodes_df = pd.read_excel('nodes.xlsx', usecols=['NodeId', 'Labels'])
nodes = nodes_df.values.tolist()

# edges file
edges_df = pd.read_excel('edges.xlsx', usecols=['sourceNodeId', 'targetNodeId'])
edges = edges_df.values.tolist()

"""### Create Directed Graph and add nodes and edges"""

G = nx.DiGraph()
G.add_nodes_from([(node_id, {'label': label}) for node_id, label in nodes])
G.add_edges_from(edges)

# Get the positions of the nodes
pos = nx.spring_layout(G, scale=50 , seed= 1234)

print(G)

"""### A. plot the graph"""

# Draw the graph
fig, ax = plt.subplots(figsize=(16, 16))
nx.draw_networkx_nodes(G, pos, node_size=10, node_color='pink', alpha=0.5, ax=ax)
nx.draw_networkx_edges(G, pos, node_size=10, edge_color='gray', arrowsize=5, alpha=0.5, ax=ax)
nx.draw_networkx_labels(G, pos, labels={node_id: label for node_id, label in nodes}, font_size=5, font_family='sans-serif', ax=ax)
ax.set_axis_off()
ax.set_title('My Network', fontsize=20, fontweight='bold')

# Add legend
pink_patch = plt.plot([],[], marker="o", ms=10, ls="", mec=None, color="pink", label="Nodes")
gray_patch = plt.plot([],[], lw=2, color="lightgray", label="Edges")
plt.legend(handles=[pink_patch[0], gray_patch[0]],
           bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)

plt.show()

"""### B. Statistical Metrics"""

!pip install tabulate
from tabulate import tabulate

# Calculate the statistical information
print('Our statistics are:')
degrees = dict(G.degree())
total_degree = sum(degrees.values())
print(f'Total Degree: {total_degree}')

total_edges = G.number_of_edges()
print(f'Total Edges: {total_edges}')

average_degree = total_degree / len(degrees)
print(f'Average Degree: {average_degree}')

# Find strongly connected components
components = list(nx.strongly_connected_components(G))

# Compute the average shortest path length
if len(components) == 1:
    # If the graph is strongly connected, calculate the average shortest path directly
    average_shortest_path = nx.average_shortest_path_length(G)
else:
    # If the graph is not strongly connected, calculate the average shortest path for each component
    component_lengths = []
    for component in components:
        subgraph = G.subgraph(component)
        if not nx.is_weakly_connected(subgraph):
            # If the component is not weakly connected, skip it
            continue
        component_lengths.append(nx.average_shortest_path_length(subgraph))
    if len(component_lengths) == 0:
        # If none of the components are weakly connected, set the average shortest path to NaN
        average_shortest_path = np.nan
    else:
        # Otherwise, compute the average shortest path across all weakly connected components
        average_shortest_path = sum(component_lengths) / len(component_lengths)
print(f'Average shortest path: {average_shortest_path}')

transitivity = nx.transitivity(G)
print(f'Transitivity: {transitivity}')

clustering_coef = nx.average_clustering(G)
print(f'Clustering Coefficient: {clustering_coef}')

density = nx.density(G)
print(f'Density: {density}')

diameter = max(max(j.values()) for i, j in nx.shortest_path_length(G))
print(f'Diameter: {diameter}')

assortativity = nx.degree_assortativity_coefficient(G)
print(f'Assortativity: {assortativity}')

"""### C. Centrality Measures"""

# Compute degree centrality
degree_centrality = nx.degree_centrality(G)

# Get top five nodes by degree centrality
top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]

# Compute closeness centrality
closeness_centrality = nx.closeness_centrality(G)
# Get top five nodes by closeness centrality
top_closeness = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]

# Compute betweenness centrality
betweenness_centrality = nx.betweenness_centrality(G)
# Get top five nodes by betweenness centrality
top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:5]

number_of_nodes = G.number_of_nodes()

lowest_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[number_of_nodes-100:]
lowest_closeness = sorted(closeness_centrality.items(), key=lambda x: x[1], reverse=True)[number_of_nodes-100:]
lowest_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[number_of_nodes-100:]

lowest_degree

lowest_closeness

lowest_betweenness

# Print the top 5 nodes by degree centrality in a table
table_degree = []
for node, centrality in top_degree:
    table_degree.append([node, centrality])
print(tabulate(table_degree, headers=['Node', 'Degree Centrality'], tablefmt='fancy_grid'))

# Print the top 5 nodes by closeness centrality in a table
table_closeness = []
for node, centrality in top_closeness:
    table_closeness.append([node, centrality])
print(tabulate(table_closeness, headers=['Node', 'Closeness Centrality'], tablefmt='fancy_grid'))

# Print the top 5 nodes by betweenness centrality in a table
table_betweenness = []
for node, centrality in top_betweenness:
    table_betweenness.append([node, centrality])
print(tabulate(table_betweenness, headers=['Node', 'Betweenness Centrality'], tablefmt='fancy_grid'))

"""# Step 2

### The most important nodes
"""

# Calculate degree distribution
degrees = [G.degree(n) for n in G.nodes()]
unique_degrees, counts = np.unique(degrees, return_counts=True)

# Create plot
fig, ax = plt.subplots(figsize=(8, 6))
ax.scatter(unique_degrees, counts, s=30, alpha=0.7, color='pink')

ax.set_xlabel('Degree', fontsize=10)
ax.set_ylabel('Count', fontsize=10)
ax.set_title('Power Law Degree Distribution', fontsize=16, fontweight='bold')
ax.tick_params(axis='both', which='major', labelsize=8)
ax.grid(axis='both', alpha=0.3)

x = unique_degrees
y = counts

slope, intercept = np.polyfit(np.log(x), np.log (y), 1)

# Calculate alpha
alpha = -1 - slope
print('Estimated alpha:', alpha)

plt.tight_layout()
plt.show()

"""Degree centrality"""

# Print the top 5 nodes by degree centrality
table = []
for node, centrality in top_degree:
    table.append([node, G.degree(node)])
print(tabulate(table, headers=['Node', 'Degree'], tablefmt='fancy_grid'))

"""Clustering Coefficient"""

clustering_coefficient = nx.clustering(G)

top_clustering = sorted(clustering_coefficient.items(), key=lambda x: x[1], reverse=True)[:5]

# Print the top 5 nodes by clustering coefficient in a table
table = []
for node, coefficient in top_clustering:
    table.append([node, f'{coefficient:.1f}'])
print(tabulate(table, headers=['Node', 'Clustering Coefficient'], tablefmt='fancy_grid'))

"""Page Rank"""

# Compute the PageRank scores
pagerank_scores = nx.pagerank(G)

# Sort the nodes by their PageRank score and print the top 5
top_pagerank_scores = sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True)[:5]

# Print the top 5 nodes by pagerank_scores in a table
table = []
for node, pagerank in top_pagerank_scores:
    table.append([node, f'{pagerank:.4f}'])
print(tabulate(table, headers=['Node', 'Page Rank'], tablefmt='fancy_grid'))

"""# Step 3

Coloring node with their label
"""

# Get the node labels
labels = nx.get_node_attributes(G, "label")

# Set the colors for each label
labels_colors = {
    'L1': 'green','L2': 'blue','L3': 'purple','L4': 'red','L5': 'orange','L6': 'pink','L7': 'Cyan','Unknown': 'black',}

# Draw the network
fig = plt.figure(figsize=(10, 10))
nodes = nx.draw_networkx_nodes(G, pos, node_color=[labels_colors[labels[node]] for node in G.nodes()], node_size=10, alpha=0.8)
edges = nx.draw_networkx_edges(G, pos, width=1, alpha=0.5, arrows=True, arrowstyle='->', arrowsize=10, node_size=10, edge_color='gray')

# Create a legend for the node colors
legend_handles = []
for label, color in labels_colors.items():
    legend_handles.append(plt.Line2D([0], [0], marker='o', color='w', label=label, markerfacecolor=color, markersize=8))
plt.legend(handles=legend_handles, loc='upper left')

plt.axis('off')
plt.show()

"""### Community detection louvain method"""

# Community detection using louvain
louvain_communities = nx.community.louvain_communities(G, seed=1234)
Q = modularity(G, louvain_communities)

print(f"Modularity of communities using louvain: {Q}")

print(louvain_communities)

len(louvain_communities)

"""### Community detection greedy method"""

# find communities by greedy method
greedy_communities = greedy_modularity_communities(G, weight=None, resolution=1, cutoff=1, best_n=None)

# Compute the modularity of the community structure
Q = modularity(G, greedy_communities)

print(f"Modularity of communities using greedy modularity optimization: {Q}")

len(greedy_communities)

"""### Is it possible to classify nodes based on identified clusters using community detection methods?"""

# classifying labels using community detection
Labels_nodes = {label: [node for node in G.nodes() if labels[node] == label] for label in set(labels.values())}

# Modularity of communitis using labels
Label_based_communities = [set(nodes) for nodes in Labels_nodes.values()]
Q = modularity(G, Label_based_communities)

print(f"Modularity of communities using labels: {Q}")

Label_based_communities

len(Label_based_communities)

"""# Step 4

### Most suspicious people
"""

# Define the SI model

def si_model(G, initial_infected_nodes, draw_flag, p=0.3):
    infected_nodes = set(initial_infected_nodes)
    susceptible_nodes = set(G.nodes()) - infected_nodes
    time = [0]
    infected_counts = [len(infected_nodes)]
    susceptible_counts = [len(susceptible_nodes)]
    new_infected_nodes_step = []
    while True:
        new_infections = set()
        for infected_node in infected_nodes:
            neighbors = set(G.neighbors(infected_node))
            susceptible_neighbors = neighbors & susceptible_nodes
            for susceptible_neighbor in susceptible_neighbors:
                if random.random() < p:
                    new_infections.add(susceptible_neighbor)
        if len(new_infections) == 0:
            break
        infected_nodes |= new_infections
        new_infected_nodes_step.append(new_infections)
        susceptible_nodes -= new_infections
        time.append(time[-1] + 1)
        infected_counts.append(len(infected_nodes))
        susceptible_counts.append(len(susceptible_nodes))
    if draw_flag == 0:
        plt.plot(time, susceptible_counts, label='Susceptible')
        plt.plot(time, infected_counts, label='Infected')
        plt.xlabel('Time')
        plt.ylabel('Number of nodes')
        plt.legend()
        plt.show()
    return infected_nodes, new_infected_nodes_step

# Run the SI model and plot the results
initial_infected_nodes =  Labels_nodes['L1']
infected_nodes = []
new_infected_nodes = []

# 10 times simulation
for i in range(10):
    this_infected_nodes, this_new_infected_nodes_step = si_model(G, initial_infected_nodes, i)
    infected_nodes.append(this_infected_nodes)
    new_infected_nodes.append(this_new_infected_nodes_step)

node_colors = ['r' if node in initial_infected_nodes else 'g' for node in G.nodes()]
node_sizes = [15 if node in initial_infected_nodes else 5 for node in G.nodes()]
edge_widths = [2 if node in initial_infected_nodes else 0.5 for node in G.nodes()]

nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes)
nx.draw_networkx_edges(G, pos, width=edge_widths)

plt.title("SI model: infected and susceptible nodes at time 0", fontsize=10)

plt.show()

node_colors = ['r' if node in infected_nodes[0] else 'g' for node in G.nodes()]
node_sizes = [15 if node in infected_nodes[0] else 5 for node in G.nodes()]
edge_widths = [2 if node in infected_nodes[0] else 0.5 for node in G.nodes()]

nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes)
nx.draw_networkx_edges(G, pos, width=edge_widths)

plt.title("SI model: infected and susceptible nodes at the End", fontsize=10)

plt.show()

"""### Unknown label nodes that fell sick in the first sim in each step"""

print('Unknown label nodes that fell sick in the first sim in each step were:')
i = 1
for steps in new_infected_nodes[0]:
    unknown_sicks_step = [node for node in steps if node in Labels_nodes['Unknown']]
    if i >= 10:
        print(f'{i}: {unknown_sicks_step}')
    else:
        print(f'{i}:  {unknown_sicks_step}')
    i += 1

"""### In 10 iterations of the simulation the chance of Unknown nodes falling sick in the first step

"""

counts = {node: 0 for node in Labels_nodes['Unknown']}
for i in range(10):
    for node in Labels_nodes['Unknown']:
        if node in new_infected_nodes[i][0]:
            counts[node] += 1

print('In 10 iterations of the simulation the chance of nodes falling sick in the first step were:')
for node, count in counts.items():
    if count > 0:
        print(f'{node}: {(count / 10) * 100}%')

"""# Step 5

Label the unknown nodes using three strategies.

## SVM Classification
"""

# Create a dictionary mapping node IDs to labels
node_labels = dict(zip(nodes_df['NodeId'], nodes_df['Labels']))

# Create a dataset of features for each node in the graph
X = np.array([[
        node,  # node ID
        G.degree(node),  # node degree
        nx.clustering(G, node) # node clustering coefficient
    ] for node in G.nodes()])

# Create a target vector with the known labels
y_known = np.array([node_labels.get(node, 'Unknown') for node in G.nodes()])

from sklearn.model_selection import train_test_split
# Split the dataset
X_train, X_val, y_train, y_val = train_test_split(X[y_known != 'Unknown'], y_known[y_known != 'Unknown'], test_size=0.1, random_state=42)

#  Train a classification algorithm
# Create a SVM classifier with a standard scaler
clf = make_pipeline(StandardScaler(), SVC(kernel='rbf', C=1, gamma='scale'))

# Train the classifier on the training set
clf.fit(X_train[:, 1:], y_train)

# Predict the labels of the unknown nodes
y_unknown = clf.predict(X[y_known == 'Unknown'][:, 1:])

# Update the node labels with the predicted labels
node_labels.update(dict(zip(np.where(y_known == 'Unknown')[0], y_unknown)))

# Predict the labels of all the nodes in the graph
y_all = clf.predict(X[:, 1:])

# Filter the feature matrix to only include unknown nodes
X_unknown = X[y_known == 'Unknown']

# Predict the labels of the unknown nodes
y_unknown_pred = clf.predict(X_unknown[:, 1:])

df = pd.DataFrame(list(zip(X_unknown[:, 0], y_unknown_pred)), columns=["NodeId", "Labels"])
df.to_csv('Unknown Labels with SVM.csv', index=False)
df

"""### Evaluate the performance of the algorithm on the validation set."""

# Predict the labels of the validation set
y_pred = clf.predict(X_val[:, 1:])

# Print the classification report
print(classification_report(y_val, y_pred))

"""## SVM using node2vec"""

!pip install node2vec
from node2vec import Node2Vec

# create vectore of graph using node2vec

node2vec = Node2Vec(G, dimensions=256, walk_length=60, num_walks=20, workers=5)
model = node2vec.fit(window=10)

# Extract the node features and labels from the graph
node_features = [model.wv[str(node)] for node in G.nodes if G.nodes[node]["label"] != "Unknown"]
node_labels = [G.nodes[node]["label"] for node in G.nodes if G.nodes[node]["label"] != "Unknown"]

# Train an SVM classifier on the labeled nodes
svm_classifier = SVC(kernel='rbf')
svm_classifier.fit(node_features, node_labels)

# Predict the labels of the unknown nodes
unknown_nodes = Labels_nodes['Unknown']
unknown_node_features = [model.wv[str(node)] for node in unknown_nodes]
unknown_node_labels = svm_classifier.predict(unknown_node_features)

# Save the predicted labels to a CSV file
df = pd.DataFrame({'NodeId': unknown_nodes, 'Labels': unknown_node_labels})
df.to_csv('Uknown Labels with SVM & Node2Vec.csv', index=False)
df

"""## DecisionTree Classification using one hot encoding"""

! pip install scikit-learn
from sklearn.tree import DecisionTreeClassifier

# Separate labeled nodes from unlabeled nodes
labeled_nodes_df = nodes_df[nodes_df["Labels"] != "Unknown"]
unlabeled_nodes_df = nodes_df[nodes_df["Labels"] == "Unknown"]

# Create feature matrix for labeled nodes
features = labeled_nodes_df[["NodeId", "Labels"]]
enc = OneHotEncoder(categories="auto", handle_unknown="ignore")
features_encoded = enc.fit_transform(features).toarray()

# Create target vector for labeled nodes
target = labeled_nodes_df["Labels"]

# Train Decision Tree on labeled nodes
clf = DecisionTreeClassifier()
clf.fit(features_encoded, target)

# Predict labels of unlabeled nodes
unlabeled_features = unlabeled_nodes_df[["NodeId", "Labels"]]
unlabeled_features_encoded = enc.transform(unlabeled_features).toarray()
predicted_labels = clf.predict(unlabeled_features_encoded)

# Update "Labels" column of nodes data frame with predicted labels
nodes_df.loc[nodes_df["Labels"] == "Unknown", "Labels"] = predicted_labels

# Print out nodes with predicted labels
print("Nodes with predicted labels:")
nodes_df.loc[nodes_df["NodeId"] >= 1135589].to_csv('Unknown Labels with Decision Tree.csv', index=False)
nodes_df.loc[nodes_df["NodeId"] >= 1135589]

"""## Naive Bayes Classification using Node2Vec"""

# Use node2vec to generate embedding vectors for each node
node_embeddings = model.wv

# Train a Naive Bayes classifier on the labeled nodes
labeled_nodes_df = nodes_df[nodes_df['Labels'] != 'Unknown']
X_train = [node_embeddings[str(node_id)] for node_id in labeled_nodes_df['NodeId']]
y_train = labeled_nodes_df['Labels']
classifier = GaussianNB()
classifier.fit(X_train, y_train)

nodes_df = pd.read_excel('nodes.xlsx', usecols=['NodeId', 'Labels'])
unknown_nodes_df= nodes_df.loc[nodes_df['Labels'] == 'Unknown']

# Predict the labels of the "Unknown" nodes
unknown_nodes_df = nodes_df[nodes_df['Labels'] == 'Unknown']
X_test = [node_embeddings[str(node_id)] for node_id in unknown_nodes_df['NodeId']]
X_test = np.array(X_test).reshape(len(X_test), -1)  # Reshape the embedding vectors
predicted_labels = classifier.predict(X_test)

# Update the labels of the "Unknown" nodes
nodes_df.loc[unknown_nodes_df.index, 'Labels'] = predicted_labels

# Print out nodes with predicted labels
print("Nodes with predicted labels:")
nodes_df.loc[nodes_df["NodeId"] >= 1135589].to_csv('Uknown Labels with Naive Bayes & Node2Vec.csv', index=False)
nodes_df.loc[nodes_df["NodeId"] >= 1135589]

